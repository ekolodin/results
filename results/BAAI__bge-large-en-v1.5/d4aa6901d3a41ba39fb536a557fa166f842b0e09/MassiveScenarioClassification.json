{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 56.431074142456055,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.38217888365837255,
        "f1": 0.3637086729457913,
        "f1_weighted": 0.3709887125137692,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.38217888365837255,
        "scores_per_experiment": [
          {
            "accuracy": 0.3910558170813719,
            "f1": 0.38229467935230854,
            "f1_weighted": 0.3788974908014846
          },
          {
            "accuracy": 0.34936112979152656,
            "f1": 0.33669475298615914,
            "f1_weighted": 0.3379198533157962
          },
          {
            "accuracy": 0.39710827168796237,
            "f1": 0.37287433650118956,
            "f1_weighted": 0.3842282017415017
          },
          {
            "accuracy": 0.3940820443846671,
            "f1": 0.37081882334807514,
            "f1_weighted": 0.3873828654688617
          },
          {
            "accuracy": 0.38063214525891054,
            "f1": 0.35467415893614507,
            "f1_weighted": 0.36007686892902624
          },
          {
            "accuracy": 0.3782784129119032,
            "f1": 0.3595705054974931,
            "f1_weighted": 0.36994766029476434
          },
          {
            "accuracy": 0.3813046402151984,
            "f1": 0.3596070159236462,
            "f1_weighted": 0.373702629254836
          },
          {
            "accuracy": 0.40248823133826495,
            "f1": 0.3946962827558542,
            "f1_weighted": 0.39155378130335877
          },
          {
            "accuracy": 0.35137861466039005,
            "f1": 0.33408540585498814,
            "f1_weighted": 0.34116055931323575
          },
          {
            "accuracy": 0.3960995292535306,
            "f1": 0.37177076830205347,
            "f1_weighted": 0.3850172147148267
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.3771765863256272,
        "f1": 0.3635169861152871,
        "f1_weighted": 0.3651668357641755,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.3771765863256272,
        "scores_per_experiment": [
          {
            "accuracy": 0.3890801770782095,
            "f1": 0.38786946530965116,
            "f1_weighted": 0.38358799696389945
          },
          {
            "accuracy": 0.3487456960157403,
            "f1": 0.33975201861048165,
            "f1_weighted": 0.3402491816165025
          },
          {
            "accuracy": 0.4033448106246926,
            "f1": 0.37953020137778964,
            "f1_weighted": 0.38847041375176256
          },
          {
            "accuracy": 0.37579931136251843,
            "f1": 0.3616111997998951,
            "f1_weighted": 0.36420617031772523
          },
          {
            "accuracy": 0.3767830791933104,
            "f1": 0.365372162520146,
            "f1_weighted": 0.3533325590689296
          },
          {
            "accuracy": 0.367929168716183,
            "f1": 0.35099776147609085,
            "f1_weighted": 0.3609846347306369
          },
          {
            "accuracy": 0.3753074274471225,
            "f1": 0.3549610390671737,
            "f1_weighted": 0.36424613347091717
          },
          {
            "accuracy": 0.38957206099360553,
            "f1": 0.3846241499110977,
            "f1_weighted": 0.3730252571378179
          },
          {
            "accuracy": 0.3295622233152976,
            "f1": 0.319572804089438,
            "f1_weighted": 0.3209851109409843
          },
          {
            "accuracy": 0.41564190850959176,
            "f1": 0.39087905899110686,
            "f1_weighted": 0.40258089964257987
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}