{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 57.44345021247864,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.4203093476798925,
        "f1": 0.4134288100957314,
        "f1_weighted": 0.42118817795638935,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.4203093476798925,
        "scores_per_experiment": [
          {
            "accuracy": 0.4428379287155346,
            "f1": 0.4397954879703997,
            "f1_weighted": 0.4384930897408912
          },
          {
            "accuracy": 0.4011432414256893,
            "f1": 0.39021688796430076,
            "f1_weighted": 0.403618162845751
          },
          {
            "accuracy": 0.43308675184936113,
            "f1": 0.42653881765330665,
            "f1_weighted": 0.43177106101792173
          },
          {
            "accuracy": 0.4250168123739072,
            "f1": 0.4130199796486607,
            "f1_weighted": 0.4237460846865297
          },
          {
            "accuracy": 0.4132481506388702,
            "f1": 0.4169895309993322,
            "f1_weighted": 0.4182838383167752
          },
          {
            "accuracy": 0.3964357767316745,
            "f1": 0.3933983868428932,
            "f1_weighted": 0.3992622200685908
          },
          {
            "accuracy": 0.43644922663080027,
            "f1": 0.4264914072875832,
            "f1_weighted": 0.44529419926680425
          },
          {
            "accuracy": 0.4394754539340955,
            "f1": 0.42506569915291054,
            "f1_weighted": 0.4327197756079876
          },
          {
            "accuracy": 0.3853396099529254,
            "f1": 0.3829786299060687,
            "f1_weighted": 0.3856581440796547
          },
          {
            "accuracy": 0.4300605245460659,
            "f1": 0.4197932735318587,
            "f1_weighted": 0.4330352039329875
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.4221347761928184,
        "f1": 0.4211498484655257,
        "f1_weighted": 0.42145495676589856,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.4221347761928184,
        "scores_per_experiment": [
          {
            "accuracy": 0.4618789965568126,
            "f1": 0.4624932140982369,
            "f1_weighted": 0.46194769932541624
          },
          {
            "accuracy": 0.39940973930152485,
            "f1": 0.40092348353610674,
            "f1_weighted": 0.40039198931262976
          },
          {
            "accuracy": 0.4353172651254304,
            "f1": 0.4328857815875278,
            "f1_weighted": 0.43479148864404904
          },
          {
            "accuracy": 0.41269060501721594,
            "f1": 0.4113066741984057,
            "f1_weighted": 0.4084289115795088
          },
          {
            "accuracy": 0.42252828332513526,
            "f1": 0.4370842824025748,
            "f1_weighted": 0.4235840454143644
          },
          {
            "accuracy": 0.3900639449090015,
            "f1": 0.3961800858011774,
            "f1_weighted": 0.3923545321949321
          },
          {
            "accuracy": 0.4466305951795376,
            "f1": 0.43992190329883835,
            "f1_weighted": 0.4520383336634002
          },
          {
            "accuracy": 0.42597147073290703,
            "f1": 0.4183951330811652,
            "f1_weighted": 0.4121176409241173
          },
          {
            "accuracy": 0.3890801770782095,
            "f1": 0.38957698601663787,
            "f1_weighted": 0.3894786724956017
          },
          {
            "accuracy": 0.4377766847024102,
            "f1": 0.42273094063458627,
            "f1_weighted": 0.439416254104966
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}