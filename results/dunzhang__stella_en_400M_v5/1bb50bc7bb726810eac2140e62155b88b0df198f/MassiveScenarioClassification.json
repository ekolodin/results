{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 89.19464540481567,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.25581708137188974,
        "f1": 0.24791678115855142,
        "f1_weighted": 0.24233995545325743,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.25581708137188974,
        "scores_per_experiment": [
          {
            "accuracy": 0.26698049764626763,
            "f1": 0.2616889293337436,
            "f1_weighted": 0.270465588702382
          },
          {
            "accuracy": 0.2824478816408877,
            "f1": 0.2662015088772411,
            "f1_weighted": 0.2776146553692399
          },
          {
            "accuracy": 0.2535305985205111,
            "f1": 0.2467491293734047,
            "f1_weighted": 0.24393712308472768
          },
          {
            "accuracy": 0.2767316745124412,
            "f1": 0.25593058524616097,
            "f1_weighted": 0.26199165890343346
          },
          {
            "accuracy": 0.22326832548755884,
            "f1": 0.2109829459631756,
            "f1_weighted": 0.18200101473617783
          },
          {
            "accuracy": 0.22562205783456624,
            "f1": 0.21151704042492317,
            "f1_weighted": 0.1950320177338689
          },
          {
            "accuracy": 0.234364492266308,
            "f1": 0.22875368996590056,
            "f1_weighted": 0.23211038461948835
          },
          {
            "accuracy": 0.285137861466039,
            "f1": 0.2806482241283775,
            "f1_weighted": 0.2656776433788408
          },
          {
            "accuracy": 0.24243443174176194,
            "f1": 0.24118605795212578,
            "f1_weighted": 0.22253806698101705
          },
          {
            "accuracy": 0.2676529926025555,
            "f1": 0.27550970032046124,
            "f1_weighted": 0.27203140102339846
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.26463354648303,
        "f1": 0.2594593881134233,
        "f1_weighted": 0.2521639527219338,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.26463354648303,
        "scores_per_experiment": [
          {
            "accuracy": 0.2867683226758485,
            "f1": 0.2854671994759544,
            "f1_weighted": 0.28747991832214115
          },
          {
            "accuracy": 0.2852926709296606,
            "f1": 0.27530194886632436,
            "f1_weighted": 0.28192149937486627
          },
          {
            "accuracy": 0.25922282341367436,
            "f1": 0.25072858551626376,
            "f1_weighted": 0.2448911657748885
          },
          {
            "accuracy": 0.28726020659124446,
            "f1": 0.2733132500420075,
            "f1_weighted": 0.2740581096429117
          },
          {
            "accuracy": 0.22429906542056074,
            "f1": 0.22104709449741577,
            "f1_weighted": 0.19461550822648138
          },
          {
            "accuracy": 0.24495818986719134,
            "f1": 0.23043421311884676,
            "f1_weighted": 0.21995713480411022
          },
          {
            "accuracy": 0.24741760944417118,
            "f1": 0.24068562479427044,
            "f1_weighted": 0.24856981196990646
          },
          {
            "accuracy": 0.27594687653713723,
            "f1": 0.26860188174011024,
            "f1_weighted": 0.25667133887061194
          },
          {
            "accuracy": 0.26512543039842595,
            "f1": 0.26620926696063824,
            "f1_weighted": 0.24396240653447399
          },
          {
            "accuracy": 0.27004426955238564,
            "f1": 0.28280481612240116,
            "f1_weighted": 0.2695126336989469
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}