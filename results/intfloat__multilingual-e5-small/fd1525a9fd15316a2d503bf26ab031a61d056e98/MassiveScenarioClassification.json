{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 273.4347870349884,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.4",
  "scores": {
    "test": [
      {
        "accuracy": 0.6274714189643579,
        "f1": 0.6232500105216028,
        "f1_weighted": 0.6177231229561672,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6274714189643579,
        "scores_per_experiment": [
          {
            "accuracy": 0.6577000672494956,
            "f1": 0.6576322752104935,
            "f1_weighted": 0.6493740224497979
          },
          {
            "accuracy": 0.6422326832548756,
            "f1": 0.6408440644199842,
            "f1_weighted": 0.6326577758515171
          },
          {
            "accuracy": 0.6472763954270343,
            "f1": 0.6305996886519358,
            "f1_weighted": 0.6425527348672134
          },
          {
            "accuracy": 0.6264290517821116,
            "f1": 0.6204764988284478,
            "f1_weighted": 0.6199108830257698
          },
          {
            "accuracy": 0.617014122394082,
            "f1": 0.603089923839662,
            "f1_weighted": 0.5966542452337902
          },
          {
            "accuracy": 0.574310692669805,
            "f1": 0.5695770003122406,
            "f1_weighted": 0.5495348145157877
          },
          {
            "accuracy": 0.6284465366509752,
            "f1": 0.6193681673903165,
            "f1_weighted": 0.6212949557775685
          },
          {
            "accuracy": 0.644250168123739,
            "f1": 0.6430675420392971,
            "f1_weighted": 0.6392111062281107
          },
          {
            "accuracy": 0.6264290517821116,
            "f1": 0.6323223501500985,
            "f1_weighted": 0.6261800264557543
          },
          {
            "accuracy": 0.6106254203093476,
            "f1": 0.6155225943735526,
            "f1_weighted": 0.5998606651563613
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6265617314313822,
        "f1": 0.6235587885501526,
        "f1_weighted": 0.6165170759513978,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6265617314313822,
        "scores_per_experiment": [
          {
            "accuracy": 0.6689621249385145,
            "f1": 0.6666305846554667,
            "f1_weighted": 0.6636893748230799
          },
          {
            "accuracy": 0.6301032956222331,
            "f1": 0.6323778007926625,
            "f1_weighted": 0.6216310538552833
          },
          {
            "accuracy": 0.6409247417609444,
            "f1": 0.6217412500421139,
            "f1_weighted": 0.6353967322731422
          },
          {
            "accuracy": 0.617314313821938,
            "f1": 0.6151965956303116,
            "f1_weighted": 0.607135576315962
          },
          {
            "accuracy": 0.6089522872602066,
            "f1": 0.5975153486980335,
            "f1_weighted": 0.5879762966147705
          },
          {
            "accuracy": 0.5636989670437776,
            "f1": 0.571808066723555,
            "f1_weighted": 0.5401520697291556
          },
          {
            "accuracy": 0.6404328578455485,
            "f1": 0.6307970090546159,
            "f1_weighted": 0.6343348109097958
          },
          {
            "accuracy": 0.6414166256763404,
            "f1": 0.6392897437434121,
            "f1_weighted": 0.634562398636677
          },
          {
            "accuracy": 0.6266601082144614,
            "f1": 0.635225572558537,
            "f1_weighted": 0.6226767108776795
          },
          {
            "accuracy": 0.6271519921298574,
            "f1": 0.625005913602817,
            "f1_weighted": 0.6176157354784336
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}