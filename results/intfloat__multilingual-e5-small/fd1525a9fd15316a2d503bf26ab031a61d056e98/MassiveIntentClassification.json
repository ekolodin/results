{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 1373.1658415794373,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.4",
  "scores": {
    "test": [
      {
        "accuracy": 0.5694687289845326,
        "f1": 0.5307147141095014,
        "f1_weighted": 0.5584758128832608,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.5694687289845326,
        "scores_per_experiment": [
          {
            "accuracy": 0.5907868190988568,
            "f1": 0.5611148056227053,
            "f1_weighted": 0.5822466034831201
          },
          {
            "accuracy": 0.5931405514458642,
            "f1": 0.5556655711302849,
            "f1_weighted": 0.590700393707666
          },
          {
            "accuracy": 0.5652320107599192,
            "f1": 0.5246905043777108,
            "f1_weighted": 0.5594738025764997
          },
          {
            "accuracy": 0.5880968392737055,
            "f1": 0.5419410324050167,
            "f1_weighted": 0.5850596839006869
          },
          {
            "accuracy": 0.5628782784129119,
            "f1": 0.5194489911828216,
            "f1_weighted": 0.5495969276682834
          },
          {
            "accuracy": 0.5332885003362475,
            "f1": 0.5190966851512107,
            "f1_weighted": 0.5195621594413293
          },
          {
            "accuracy": 0.5790181573638198,
            "f1": 0.5357590955504744,
            "f1_weighted": 0.5671428428541702
          },
          {
            "accuracy": 0.554808338937458,
            "f1": 0.509002510348647,
            "f1_weighted": 0.5393986975038177
          },
          {
            "accuracy": 0.5497646267652992,
            "f1": 0.5109282403227248,
            "f1_weighted": 0.5269291942718756
          },
          {
            "accuracy": 0.5776731674512441,
            "f1": 0.5294997050034175,
            "f1_weighted": 0.5646478234251583
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5757009345794393,
        "f1": 0.5188823893319428,
        "f1_weighted": 0.5669502392200523,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.5757009345794393,
        "scores_per_experiment": [
          {
            "accuracy": 0.5804230201672406,
            "f1": 0.5296776585193796,
            "f1_weighted": 0.573056109873572
          },
          {
            "accuracy": 0.602557796360059,
            "f1": 0.5395899047953502,
            "f1_weighted": 0.5991227258611797
          },
          {
            "accuracy": 0.5779636005902608,
            "f1": 0.5125653220780408,
            "f1_weighted": 0.5744180592969034
          },
          {
            "accuracy": 0.6000983767830792,
            "f1": 0.5443706159810089,
            "f1_weighted": 0.604142719334455
          },
          {
            "accuracy": 0.5681259222823414,
            "f1": 0.4991333663204531,
            "f1_weighted": 0.5578109649457884
          },
          {
            "accuracy": 0.5518937530742745,
            "f1": 0.5123670381359348,
            "f1_weighted": 0.5407090999681644
          },
          {
            "accuracy": 0.5715691096901131,
            "f1": 0.5153659027514236,
            "f1_weighted": 0.5589157773990072
          },
          {
            "accuracy": 0.5558288243974422,
            "f1": 0.49925779962495037,
            "f1_weighted": 0.5448447638537954
          },
          {
            "accuracy": 0.5607476635514018,
            "f1": 0.5110426859222787,
            "f1_weighted": 0.5420738292518411
          },
          {
            "accuracy": 0.58780127889818,
            "f1": 0.5254535991906079,
            "f1_weighted": 0.5744083424158165
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}