{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 210.42088508605957,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.7336583725622058,
        "f1": 0.7041486844338799,
        "f1_weighted": 0.7219001292766163,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.7336583725622058,
        "scores_per_experiment": [
          {
            "accuracy": 0.7410894418291862,
            "f1": 0.71036265711149,
            "f1_weighted": 0.7297361186854204
          },
          {
            "accuracy": 0.7471418964357768,
            "f1": 0.7096025161995269,
            "f1_weighted": 0.7382751725849083
          },
          {
            "accuracy": 0.7242770679219905,
            "f1": 0.6956569622589873,
            "f1_weighted": 0.7138835702751832
          },
          {
            "accuracy": 0.7501681237390719,
            "f1": 0.709203702226879,
            "f1_weighted": 0.7407402980045585
          },
          {
            "accuracy": 0.7407531943510424,
            "f1": 0.7087779173279923,
            "f1_weighted": 0.7300548958336911
          },
          {
            "accuracy": 0.7269670477471419,
            "f1": 0.7118757006668955,
            "f1_weighted": 0.7194101903409768
          },
          {
            "accuracy": 0.7326832548755884,
            "f1": 0.7001322294482387,
            "f1_weighted": 0.7171119326353882
          },
          {
            "accuracy": 0.7239408204438467,
            "f1": 0.683906516901999,
            "f1_weighted": 0.7110453438377231
          },
          {
            "accuracy": 0.6997310020174848,
            "f1": 0.687285247942128,
            "f1_weighted": 0.6756161892733505
          },
          {
            "accuracy": 0.7498318762609281,
            "f1": 0.7246833942546619,
            "f1_weighted": 0.7431275812949625
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.7424003935071324,
        "f1": 0.7002602727697711,
        "f1_weighted": 0.7281309209875871,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.7424003935071324,
        "scores_per_experiment": [
          {
            "accuracy": 0.7560255779636006,
            "f1": 0.7059355862386306,
            "f1_weighted": 0.7436840115840425
          },
          {
            "accuracy": 0.7570093457943925,
            "f1": 0.7061836448113953,
            "f1_weighted": 0.7460450031376
          },
          {
            "accuracy": 0.7397934087555337,
            "f1": 0.6973484287084937,
            "f1_weighted": 0.7259273846794938
          },
          {
            "accuracy": 0.7565174618789966,
            "f1": 0.70912944118439,
            "f1_weighted": 0.7420127135102002
          },
          {
            "accuracy": 0.7456960157402853,
            "f1": 0.6981902622782278,
            "f1_weighted": 0.7327054921113475
          },
          {
            "accuracy": 0.7397934087555337,
            "f1": 0.705913482623903,
            "f1_weighted": 0.7310716749391933
          },
          {
            "accuracy": 0.7397934087555337,
            "f1": 0.6917552987665041,
            "f1_weighted": 0.7226417855732911
          },
          {
            "accuracy": 0.7304476143630103,
            "f1": 0.6915723850018337,
            "f1_weighted": 0.712978333943807
          },
          {
            "accuracy": 0.7112641416625677,
            "f1": 0.6894183243112603,
            "f1_weighted": 0.6864684003227117
          },
          {
            "accuracy": 0.7476635514018691,
            "f1": 0.7071558737730729,
            "f1_weighted": 0.7377744100741843
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}