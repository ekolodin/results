{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 223.14069080352783,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.8393073301950237,
        "f1": 0.8270612568214959,
        "f1_weighted": 0.836458554157962,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.8393073301950237,
        "scores_per_experiment": [
          {
            "accuracy": 0.8446536650975117,
            "f1": 0.8343401564938051,
            "f1_weighted": 0.8416522565082213
          },
          {
            "accuracy": 0.8389374579690653,
            "f1": 0.8272335710872893,
            "f1_weighted": 0.8365457478305465
          },
          {
            "accuracy": 0.855749831876261,
            "f1": 0.8376154038060196,
            "f1_weighted": 0.8502882311876104
          },
          {
            "accuracy": 0.84364492266308,
            "f1": 0.8310131588042805,
            "f1_weighted": 0.840132036947145
          },
          {
            "accuracy": 0.8312037659717552,
            "f1": 0.8146849712213247,
            "f1_weighted": 0.8252011148113731
          },
          {
            "accuracy": 0.832212508406187,
            "f1": 0.8180613549308879,
            "f1_weighted": 0.8286765993077079
          },
          {
            "accuracy": 0.8167451244115669,
            "f1": 0.8074328041317558,
            "f1_weighted": 0.8152731387609468
          },
          {
            "accuracy": 0.8312037659717552,
            "f1": 0.8208901443290727,
            "f1_weighted": 0.8311261091003769
          },
          {
            "accuracy": 0.8544048419636853,
            "f1": 0.8410088012823086,
            "f1_weighted": 0.8513623650334743
          },
          {
            "accuracy": 0.8443174176193678,
            "f1": 0.8383322021282162,
            "f1_weighted": 0.8443279420922176
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.8373339891785537,
        "f1": 0.822707585935388,
        "f1_weighted": 0.8348319882576032,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.8373339891785537,
        "scores_per_experiment": [
          {
            "accuracy": 0.853418593212002,
            "f1": 0.8409798360229135,
            "f1_weighted": 0.8508816786505028
          },
          {
            "accuracy": 0.8342351205115592,
            "f1": 0.8228723619189575,
            "f1_weighted": 0.8332449413029882
          },
          {
            "accuracy": 0.853418593212002,
            "f1": 0.8334023659493713,
            "f1_weighted": 0.8489185401824909
          },
          {
            "accuracy": 0.8425971470732907,
            "f1": 0.8300890295106647,
            "f1_weighted": 0.8384190364514511
          },
          {
            "accuracy": 0.8293162813575996,
            "f1": 0.8127407838091611,
            "f1_weighted": 0.8247161178117364
          },
          {
            "accuracy": 0.8357107722577471,
            "f1": 0.8211888273279606,
            "f1_weighted": 0.833011470520948
          },
          {
            "accuracy": 0.8214461387112642,
            "f1": 0.8052039526396647,
            "f1_weighted": 0.8205171529928538
          },
          {
            "accuracy": 0.8184948352188883,
            "f1": 0.8042460395443892,
            "f1_weighted": 0.8185628752396399
          },
          {
            "accuracy": 0.8455484505656665,
            "f1": 0.8303422578125932,
            "f1_weighted": 0.8412225323560316
          },
          {
            "accuracy": 0.839153959665519,
            "f1": 0.8260104048182032,
            "f1_weighted": 0.8388255370673898
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}