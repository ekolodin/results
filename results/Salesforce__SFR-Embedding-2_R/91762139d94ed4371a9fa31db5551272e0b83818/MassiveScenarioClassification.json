{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 153.84353232383728,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.8866173503698723,
        "f1": 0.876931518341441,
        "f1_weighted": 0.8836447835208376,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.8866173503698723,
        "scores_per_experiment": [
          {
            "accuracy": 0.8893745796906524,
            "f1": 0.8806938477376733,
            "f1_weighted": 0.8867524728166489
          },
          {
            "accuracy": 0.8890383322125084,
            "f1": 0.8795699554734846,
            "f1_weighted": 0.8870009618129333
          },
          {
            "accuracy": 0.8897108271687962,
            "f1": 0.8771245188947406,
            "f1_weighted": 0.8847515182498809
          },
          {
            "accuracy": 0.8883658372562205,
            "f1": 0.8775038622826093,
            "f1_weighted": 0.8836300373379549
          },
          {
            "accuracy": 0.8883658372562205,
            "f1": 0.8766751475965635,
            "f1_weighted": 0.8842288608916484
          },
          {
            "accuracy": 0.8843308675184937,
            "f1": 0.8750495259305986,
            "f1_weighted": 0.8825856529574927
          },
          {
            "accuracy": 0.8651647612642905,
            "f1": 0.8583911895377763,
            "f1_weighted": 0.8633455973308025
          },
          {
            "accuracy": 0.8876933422999328,
            "f1": 0.8783105618314875,
            "f1_weighted": 0.8854289437665281
          },
          {
            "accuracy": 0.8873570948217888,
            "f1": 0.8779402589647946,
            "f1_weighted": 0.8843006643270798
          },
          {
            "accuracy": 0.8967720242098184,
            "f1": 0.8880563151646819,
            "f1_weighted": 0.8944231257174076
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.887063453025086,
        "f1": 0.875692375325753,
        "f1_weighted": 0.8839357197993657,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.887063453025086,
        "scores_per_experiment": [
          {
            "accuracy": 0.8903098868666994,
            "f1": 0.8812939993632483,
            "f1_weighted": 0.888240395077318
          },
          {
            "accuracy": 0.8898180029513035,
            "f1": 0.8789138841066256,
            "f1_weighted": 0.8880508154603761
          },
          {
            "accuracy": 0.8868666994589277,
            "f1": 0.8722701612342092,
            "f1_weighted": 0.8812424561538283
          },
          {
            "accuracy": 0.8908017707820954,
            "f1": 0.8786594783059305,
            "f1_weighted": 0.8860854192528625
          },
          {
            "accuracy": 0.8888342351205115,
            "f1": 0.8757361660188668,
            "f1_weighted": 0.8850271288305297
          },
          {
            "accuracy": 0.8868666994589277,
            "f1": 0.8766830940180063,
            "f1_weighted": 0.8849564506901489
          },
          {
            "accuracy": 0.8647319232661091,
            "f1": 0.8558049265589309,
            "f1_weighted": 0.8622079726331933
          },
          {
            "accuracy": 0.8893261190359075,
            "f1": 0.8778794492077554,
            "f1_weighted": 0.8863612540774493
          },
          {
            "accuracy": 0.8893261190359075,
            "f1": 0.8776864752873723,
            "f1_weighted": 0.885773367830472
          },
          {
            "accuracy": 0.8937530742744713,
            "f1": 0.8819961191565838,
            "f1_weighted": 0.8914119379874779
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}