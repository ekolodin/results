{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 20.951470613479614,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.85",
  "scores": {
    "test": [
      {
        "accuracy": 0.6592468056489577,
        "f1": 0.6332930220303818,
        "f1_weighted": 0.6508443105041003,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6592468056489577,
        "scores_per_experiment": [
          {
            "accuracy": 0.6772024209818427,
            "f1": 0.6500098054988735,
            "f1_weighted": 0.6712776417318309
          },
          {
            "accuracy": 0.6866173503698723,
            "f1": 0.6586748551381465,
            "f1_weighted": 0.6803276161338214
          },
          {
            "accuracy": 0.6466039004707465,
            "f1": 0.6234221619344356,
            "f1_weighted": 0.6392532910780089
          },
          {
            "accuracy": 0.6809011432414257,
            "f1": 0.6429988453339669,
            "f1_weighted": 0.6766179041247198
          },
          {
            "accuracy": 0.6711499663752521,
            "f1": 0.6282120171695812,
            "f1_weighted": 0.6571848243839463
          },
          {
            "accuracy": 0.6116341627437795,
            "f1": 0.6023394971110142,
            "f1_weighted": 0.6084770533954141
          },
          {
            "accuracy": 0.6449226630800269,
            "f1": 0.6265072171676476,
            "f1_weighted": 0.6312289797130667
          },
          {
            "accuracy": 0.6476126429051782,
            "f1": 0.6196337549636551,
            "f1_weighted": 0.638569614574751
          },
          {
            "accuracy": 0.6489576328177539,
            "f1": 0.6273829161739766,
            "f1_weighted": 0.6364352110265142
          },
          {
            "accuracy": 0.6768661735036987,
            "f1": 0.65374914981252,
            "f1_weighted": 0.6690709688789301
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6599114608952287,
        "f1": 0.6229951814190612,
        "f1_weighted": 0.6515177972982181,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6599114608952287,
        "scores_per_experiment": [
          {
            "accuracy": 0.6733890801770782,
            "f1": 0.6440174924035045,
            "f1_weighted": 0.669484695114143
          },
          {
            "accuracy": 0.690113133300541,
            "f1": 0.6484248161401168,
            "f1_weighted": 0.6880499764521711
          },
          {
            "accuracy": 0.6684702410231186,
            "f1": 0.6326134051706417,
            "f1_weighted": 0.6607817737157313
          },
          {
            "accuracy": 0.6832267584849976,
            "f1": 0.6386553370956893,
            "f1_weighted": 0.6803230255098757
          },
          {
            "accuracy": 0.6596163305459911,
            "f1": 0.5910124014439783,
            "f1_weighted": 0.6446930950348159
          },
          {
            "accuracy": 0.6207575012297097,
            "f1": 0.603298155224133,
            "f1_weighted": 0.6104857701116787
          },
          {
            "accuracy": 0.6345302508607968,
            "f1": 0.6062149764255728,
            "f1_weighted": 0.6184320065392511
          },
          {
            "accuracy": 0.6433841613379242,
            "f1": 0.6090008557567135,
            "f1_weighted": 0.6351164935117489
          },
          {
            "accuracy": 0.6502705361534677,
            "f1": 0.6218459450015268,
            "f1_weighted": 0.6405272983462579
          },
          {
            "accuracy": 0.675356615838662,
            "f1": 0.634868429528735,
            "f1_weighted": 0.6672838386465079
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}