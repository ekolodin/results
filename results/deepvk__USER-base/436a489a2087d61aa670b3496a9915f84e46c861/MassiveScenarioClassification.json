{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 12.991098165512085,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.85",
  "scores": {
    "test": [
      {
        "accuracy": 0.6906186953597848,
        "f1": 0.6866308033968759,
        "f1_weighted": 0.6857808586537245,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6906186953597848,
        "scores_per_experiment": [
          {
            "accuracy": 0.6943510423671823,
            "f1": 0.6956218548560213,
            "f1_weighted": 0.6882126592890419
          },
          {
            "accuracy": 0.7088096839273705,
            "f1": 0.7047826005245202,
            "f1_weighted": 0.7020026346057006
          },
          {
            "accuracy": 0.7044384667114997,
            "f1": 0.6945882647057474,
            "f1_weighted": 0.7014520494294398
          },
          {
            "accuracy": 0.6882985877605918,
            "f1": 0.685481174275943,
            "f1_weighted": 0.6857969859523981
          },
          {
            "accuracy": 0.6906523201075991,
            "f1": 0.6811043694536849,
            "f1_weighted": 0.6831180352162991
          },
          {
            "accuracy": 0.6647612642905179,
            "f1": 0.6603349453338198,
            "f1_weighted": 0.6568200721651536
          },
          {
            "accuracy": 0.6893073301950235,
            "f1": 0.682045802868291,
            "f1_weighted": 0.6858794077890968
          },
          {
            "accuracy": 0.6761936785474109,
            "f1": 0.677670424643281,
            "f1_weighted": 0.6740623573484348
          },
          {
            "accuracy": 0.7078009414929388,
            "f1": 0.7070321509364649,
            "f1_weighted": 0.7061897600267596
          },
          {
            "accuracy": 0.6815736381977135,
            "f1": 0.6776464463709849,
            "f1_weighted": 0.6742746247149205
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6871618298081653,
        "f1": 0.6813267871979182,
        "f1_weighted": 0.6829653996893705,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6871618298081653,
        "scores_per_experiment": [
          {
            "accuracy": 0.7043777668470241,
            "f1": 0.6979482540228524,
            "f1_weighted": 0.7029182184820072
          },
          {
            "accuracy": 0.6984751598622725,
            "f1": 0.6917065413502284,
            "f1_weighted": 0.6936176411951938
          },
          {
            "accuracy": 0.7151992129857354,
            "f1": 0.707958668914985,
            "f1_weighted": 0.7141162700925685
          },
          {
            "accuracy": 0.677816035415642,
            "f1": 0.6761573175170243,
            "f1_weighted": 0.6754785615031049
          },
          {
            "accuracy": 0.690113133300541,
            "f1": 0.6832732302226258,
            "f1_weighted": 0.6814574028127867
          },
          {
            "accuracy": 0.6537137235612396,
            "f1": 0.6471686484392954,
            "f1_weighted": 0.6458431068892121
          },
          {
            "accuracy": 0.6738809640924742,
            "f1": 0.6601962857775775,
            "f1_weighted": 0.6706001983981201
          },
          {
            "accuracy": 0.6669945892769307,
            "f1": 0.6655781960376382,
            "f1_weighted": 0.6625443441785197
          },
          {
            "accuracy": 0.7009345794392523,
            "f1": 0.7024633875587701,
            "f1_weighted": 0.6979562052486792
          },
          {
            "accuracy": 0.690113133300541,
            "f1": 0.6808173421381848,
            "f1_weighted": 0.6851220480935127
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}