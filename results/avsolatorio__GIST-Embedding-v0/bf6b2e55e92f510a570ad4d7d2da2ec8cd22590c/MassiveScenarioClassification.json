{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 32.806586265563965,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.3642905178211163,
        "f1": 0.3392313193902316,
        "f1_weighted": 0.3457909204677908,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.3642905178211163,
        "scores_per_experiment": [
          {
            "accuracy": 0.36583725622057833,
            "f1": 0.34697182901572,
            "f1_weighted": 0.3444778893143892
          },
          {
            "accuracy": 0.34667114996637527,
            "f1": 0.3218448766016233,
            "f1_weighted": 0.3235188627294629
          },
          {
            "accuracy": 0.3762609280430397,
            "f1": 0.3447620687696501,
            "f1_weighted": 0.35923586608789043
          },
          {
            "accuracy": 0.37491593813046403,
            "f1": 0.34340425800102575,
            "f1_weighted": 0.3629645842512604
          },
          {
            "accuracy": 0.3624747814391392,
            "f1": 0.33789469966393226,
            "f1_weighted": 0.3410742050031575
          },
          {
            "accuracy": 0.34700739744451914,
            "f1": 0.3187169018129359,
            "f1_weighted": 0.3224815754888547
          },
          {
            "accuracy": 0.36617350369872226,
            "f1": 0.34003207469873936,
            "f1_weighted": 0.3507067460546692
          },
          {
            "accuracy": 0.3695359784801614,
            "f1": 0.35260055838847126,
            "f1_weighted": 0.34979385561417387
          },
          {
            "accuracy": 0.34667114996637527,
            "f1": 0.32329452984220336,
            "f1_weighted": 0.32982637460853054
          },
          {
            "accuracy": 0.3873570948217888,
            "f1": 0.3627913971080149,
            "f1_weighted": 0.3738292455255192
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.361829808165273,
        "f1": 0.34162649825507574,
        "f1_weighted": 0.3405604655066674,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.361829808165273,
        "scores_per_experiment": [
          {
            "accuracy": 0.35809149040826366,
            "f1": 0.34427396665739296,
            "f1_weighted": 0.33587068047918206
          },
          {
            "accuracy": 0.33841613379242497,
            "f1": 0.32088229779804217,
            "f1_weighted": 0.3109422049945239
          },
          {
            "accuracy": 0.38465322183964584,
            "f1": 0.35765341742457113,
            "f1_weighted": 0.36589417320851175
          },
          {
            "accuracy": 0.3753074274471225,
            "f1": 0.34409702944396664,
            "f1_weighted": 0.3580173340783647
          },
          {
            "accuracy": 0.35809149040826366,
            "f1": 0.3401300534414439,
            "f1_weighted": 0.3299072352960808
          },
          {
            "accuracy": 0.3497294638465322,
            "f1": 0.3217344696595402,
            "f1_weighted": 0.33504191455611426
          },
          {
            "accuracy": 0.3630103295622233,
            "f1": 0.3445320691979703,
            "f1_weighted": 0.34178713227306284
          },
          {
            "accuracy": 0.3630103295622233,
            "f1": 0.34704929995226985,
            "f1_weighted": 0.33742996897777866
          },
          {
            "accuracy": 0.3364485981308411,
            "f1": 0.32239065609098055,
            "f1_weighted": 0.31777840934389723
          },
          {
            "accuracy": 0.3915395966551894,
            "f1": 0.3735217228845796,
            "f1_weighted": 0.372935601859158
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}