{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 56.10311555862427,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.31281102891728313,
        "f1": 0.2818318390855832,
        "f1_weighted": 0.2883537570593143,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.31281102891728313,
        "scores_per_experiment": [
          {
            "accuracy": 0.3238063214525891,
            "f1": 0.3016677606458697,
            "f1_weighted": 0.294376122124558
          },
          {
            "accuracy": 0.312373907195696,
            "f1": 0.2815534109757773,
            "f1_weighted": 0.28958248160296896
          },
          {
            "accuracy": 0.3069939475453934,
            "f1": 0.2900615759603937,
            "f1_weighted": 0.28879043197553805
          },
          {
            "accuracy": 0.30497646267652995,
            "f1": 0.2640428099021538,
            "f1_weighted": 0.28412731116719364
          },
          {
            "accuracy": 0.3406186953597848,
            "f1": 0.295183195352672,
            "f1_weighted": 0.31604625004386205
          },
          {
            "accuracy": 0.2915265635507734,
            "f1": 0.25587550204153986,
            "f1_weighted": 0.27534843547064186
          },
          {
            "accuracy": 0.31002017484868866,
            "f1": 0.2863754962632543,
            "f1_weighted": 0.2859610128278995
          },
          {
            "accuracy": 0.3073301950235373,
            "f1": 0.2713466784530624,
            "f1_weighted": 0.27772235105704474
          },
          {
            "accuracy": 0.2965702757229321,
            "f1": 0.27179307168919714,
            "f1_weighted": 0.26429254424396653
          },
          {
            "accuracy": 0.3338937457969065,
            "f1": 0.30041888957191176,
            "f1_weighted": 0.30729063007946944
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.3122970978848992,
        "f1": 0.2767086067308245,
        "f1_weighted": 0.29046449249745737,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.3122970978848992,
        "scores_per_experiment": [
          {
            "accuracy": 0.30545991146089524,
            "f1": 0.2793609033189319,
            "f1_weighted": 0.2785324900738541
          },
          {
            "accuracy": 0.3034923757993114,
            "f1": 0.26847102018667274,
            "f1_weighted": 0.2801336556934657
          },
          {
            "accuracy": 0.3157894736842105,
            "f1": 0.2899203435341625,
            "f1_weighted": 0.3053547570327551
          },
          {
            "accuracy": 0.30644367929168714,
            "f1": 0.2618600980299359,
            "f1_weighted": 0.2828577127510715
          },
          {
            "accuracy": 0.33448106246925724,
            "f1": 0.2813645133665386,
            "f1_weighted": 0.31142120822248376
          },
          {
            "accuracy": 0.2985735366453517,
            "f1": 0.26918388107809854,
            "f1_weighted": 0.2842832098289532
          },
          {
            "accuracy": 0.32267584849975406,
            "f1": 0.29020604439146963,
            "f1_weighted": 0.3022471383931498
          },
          {
            "accuracy": 0.30644367929168714,
            "f1": 0.26468556529101467,
            "f1_weighted": 0.28394335459496545
          },
          {
            "accuracy": 0.2951303492375799,
            "f1": 0.2745526838876486,
            "f1_weighted": 0.2673737181399241
          },
          {
            "accuracy": 0.33448106246925724,
            "f1": 0.2874810142237714,
            "f1_weighted": 0.3084976802439507
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}