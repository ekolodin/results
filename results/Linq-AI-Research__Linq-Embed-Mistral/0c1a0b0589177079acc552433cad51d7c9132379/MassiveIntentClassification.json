{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 283.396684885025,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.7718560860793544,
        "f1": 0.7383036343098989,
        "f1_weighted": 0.7638305290060587,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.7718560860793544,
        "scores_per_experiment": [
          {
            "accuracy": 0.777404169468729,
            "f1": 0.7370559854660887,
            "f1_weighted": 0.7687330709415543
          },
          {
            "accuracy": 0.7814391392064559,
            "f1": 0.7523363796778161,
            "f1_weighted": 0.7736351212857856
          },
          {
            "accuracy": 0.7669804976462676,
            "f1": 0.7365878795333431,
            "f1_weighted": 0.7582574042872109
          },
          {
            "accuracy": 0.7868190988567586,
            "f1": 0.7451092096689428,
            "f1_weighted": 0.7808337872053428
          },
          {
            "accuracy": 0.7700067249495629,
            "f1": 0.7299475112085703,
            "f1_weighted": 0.7589373993372903
          },
          {
            "accuracy": 0.7763954270342972,
            "f1": 0.7463430197275412,
            "f1_weighted": 0.7748130239655857
          },
          {
            "accuracy": 0.7679892400806994,
            "f1": 0.7255555394608271,
            "f1_weighted": 0.7527355163233992
          },
          {
            "accuracy": 0.7683254875588433,
            "f1": 0.7375037895749078,
            "f1_weighted": 0.7636723561469657
          },
          {
            "accuracy": 0.7498318762609281,
            "f1": 0.724030717719674,
            "f1_weighted": 0.7405912844270286
          },
          {
            "accuracy": 0.773369199731002,
            "f1": 0.7485663110612776,
            "f1_weighted": 0.7660963261404229
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.7791933103787506,
        "f1": 0.737426180259194,
        "f1_weighted": 0.7666381598228467,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.7791933103787506,
        "scores_per_experiment": [
          {
            "accuracy": 0.7845548450565667,
            "f1": 0.7440596664845416,
            "f1_weighted": 0.7722615119198849
          },
          {
            "accuracy": 0.7909493359567142,
            "f1": 0.7488630037694928,
            "f1_weighted": 0.7792529414980506
          },
          {
            "accuracy": 0.778160354156419,
            "f1": 0.7355764453222771,
            "f1_weighted": 0.7661055930142987
          },
          {
            "accuracy": 0.7860304968027545,
            "f1": 0.7481907763117869,
            "f1_weighted": 0.7732067379415821
          },
          {
            "accuracy": 0.7840629611411707,
            "f1": 0.7368718426066329,
            "f1_weighted": 0.7715342421121536
          },
          {
            "accuracy": 0.7884899163797344,
            "f1": 0.7413749574303281,
            "f1_weighted": 0.781955359702065
          },
          {
            "accuracy": 0.7791441219872111,
            "f1": 0.7255138457468365,
            "f1_weighted": 0.7608562119849925
          },
          {
            "accuracy": 0.764387604525332,
            "f1": 0.7282226448768303,
            "f1_weighted": 0.7523528285468242
          },
          {
            "accuracy": 0.7540580423020167,
            "f1": 0.7219137680621117,
            "f1_weighted": 0.7369730678516829
          },
          {
            "accuracy": 0.7820954254795868,
            "f1": 0.743674851981103,
            "f1_weighted": 0.7718831036569318
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}