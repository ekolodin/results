{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 260.02703046798706,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.7744115669132481,
        "f1": 0.764196629760842,
        "f1_weighted": 0.7713290137945937,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.7744115669132481,
        "scores_per_experiment": [
          {
            "accuracy": 0.7726967047747142,
            "f1": 0.7669945395156134,
            "f1_weighted": 0.7720271054793457
          },
          {
            "accuracy": 0.7723604572965703,
            "f1": 0.7606743132712874,
            "f1_weighted": 0.7689918576400112
          },
          {
            "accuracy": 0.7804303967720242,
            "f1": 0.7648004681415664,
            "f1_weighted": 0.774707364167859
          },
          {
            "accuracy": 0.7992602555480834,
            "f1": 0.7822413744969521,
            "f1_weighted": 0.7953495024375533
          },
          {
            "accuracy": 0.7763954270342972,
            "f1": 0.7599806982283944,
            "f1_weighted": 0.7692252873375505
          },
          {
            "accuracy": 0.7407531943510424,
            "f1": 0.7374525855410359,
            "f1_weighted": 0.737498432059467
          },
          {
            "accuracy": 0.765635507733692,
            "f1": 0.7547839902068046,
            "f1_weighted": 0.7604861528059819
          },
          {
            "accuracy": 0.7723604572965703,
            "f1": 0.7696353007388201,
            "f1_weighted": 0.7754282367175047
          },
          {
            "accuracy": 0.7948890383322125,
            "f1": 0.7831461824895359,
            "f1_weighted": 0.7925798195947932
          },
          {
            "accuracy": 0.769334229993275,
            "f1": 0.7622568449784084,
            "f1_weighted": 0.7669963797058712
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.769749139203148,
        "f1": 0.7558298375150284,
        "f1_weighted": 0.7676718588835427,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.769749139203148,
        "scores_per_experiment": [
          {
            "accuracy": 0.7668470241023119,
            "f1": 0.7568565705375698,
            "f1_weighted": 0.7669253839197117
          },
          {
            "accuracy": 0.7668470241023119,
            "f1": 0.756079301369588,
            "f1_weighted": 0.766920440971579
          },
          {
            "accuracy": 0.7865223807181505,
            "f1": 0.7699808718951124,
            "f1_weighted": 0.781526953131049
          },
          {
            "accuracy": 0.7968519429414658,
            "f1": 0.7760660919266599,
            "f1_weighted": 0.7925657688723198
          },
          {
            "accuracy": 0.7757009345794392,
            "f1": 0.7520749126939,
            "f1_weighted": 0.7694388847482563
          },
          {
            "accuracy": 0.7324151500245942,
            "f1": 0.7268934507203584,
            "f1_weighted": 0.7286901163718955
          },
          {
            "accuracy": 0.750122970978849,
            "f1": 0.7329980036543031,
            "f1_weighted": 0.7470975243407222
          },
          {
            "accuracy": 0.7624200688637481,
            "f1": 0.7535502440606934,
            "f1_weighted": 0.7667031190635183
          },
          {
            "accuracy": 0.794884407279882,
            "f1": 0.7837787411206423,
            "f1_weighted": 0.7929389443712709
          },
          {
            "accuracy": 0.764879488440728,
            "f1": 0.7500201871714574,
            "f1_weighted": 0.7639114530451045
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}