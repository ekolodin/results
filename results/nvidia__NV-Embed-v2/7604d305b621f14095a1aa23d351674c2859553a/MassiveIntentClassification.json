{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 471.122656583786,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.7178547410894419,
        "f1": 0.6898441940684256,
        "f1_weighted": 0.7136645623324196,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.7178547410894419,
        "scores_per_experiment": [
          {
            "accuracy": 0.7441156691324815,
            "f1": 0.7133171277889675,
            "f1_weighted": 0.7387104144834933
          },
          {
            "accuracy": 0.7256220578345662,
            "f1": 0.6933669519501607,
            "f1_weighted": 0.727884098368525
          },
          {
            "accuracy": 0.7135171486213854,
            "f1": 0.6881913353888739,
            "f1_weighted": 0.7135193775406943
          },
          {
            "accuracy": 0.7387357094821789,
            "f1": 0.7003035026152659,
            "f1_weighted": 0.7377064355413773
          },
          {
            "accuracy": 0.7195696032279758,
            "f1": 0.6899397668752968,
            "f1_weighted": 0.709380063906068
          },
          {
            "accuracy": 0.6886348352387357,
            "f1": 0.6669694520307593,
            "f1_weighted": 0.6866477598893861
          },
          {
            "accuracy": 0.718897108271688,
            "f1": 0.6961703486673134,
            "f1_weighted": 0.7154700307918725
          },
          {
            "accuracy": 0.6967047747141897,
            "f1": 0.6667140760203905,
            "f1_weighted": 0.6891539365898484
          },
          {
            "accuracy": 0.703093476798924,
            "f1": 0.6842290362288688,
            "f1_weighted": 0.6914601101611271
          },
          {
            "accuracy": 0.7296570275722932,
            "f1": 0.6992403431183593,
            "f1_weighted": 0.7267133960518033
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.7141662567634038,
        "f1": 0.6732004745587511,
        "f1_weighted": 0.7077613503749703,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.7141662567634038,
        "scores_per_experiment": [
          {
            "accuracy": 0.7442203639940974,
            "f1": 0.7127082157245627,
            "f1_weighted": 0.7367187747608305
          },
          {
            "accuracy": 0.720118052139695,
            "f1": 0.6679715910682619,
            "f1_weighted": 0.7189188447367377
          },
          {
            "accuracy": 0.7260206591244466,
            "f1": 0.6852257248723899,
            "f1_weighted": 0.7236805624917242
          },
          {
            "accuracy": 0.7289719626168224,
            "f1": 0.6776079380888448,
            "f1_weighted": 0.7239346578102468
          },
          {
            "accuracy": 0.7127397934087555,
            "f1": 0.667175647871863,
            "f1_weighted": 0.7027077995239003
          },
          {
            "accuracy": 0.6960157402852927,
            "f1": 0.6641490689139968,
            "f1_weighted": 0.6939169784647554
          },
          {
            "accuracy": 0.7097884899163798,
            "f1": 0.6742290115691805,
            "f1_weighted": 0.7017467666210515
          },
          {
            "accuracy": 0.6876537137235612,
            "f1": 0.6462768028707662,
            "f1_weighted": 0.6793417095045327
          },
          {
            "accuracy": 0.6871618298081653,
            "f1": 0.6566338882738231,
            "f1_weighted": 0.6697367530917898
          },
          {
            "accuracy": 0.7289719626168224,
            "f1": 0.6800268563338224,
            "f1_weighted": 0.7269106567441339
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}