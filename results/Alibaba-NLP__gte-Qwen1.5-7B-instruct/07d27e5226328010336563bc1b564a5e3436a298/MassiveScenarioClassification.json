{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 133.87348747253418,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.7545729657027572,
        "f1": 0.7490898691066853,
        "f1_weighted": 0.7515855273153575,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.7545729657027572,
        "scores_per_experiment": [
          {
            "accuracy": 0.7511768661735037,
            "f1": 0.7456108309189888,
            "f1_weighted": 0.7484723005511253
          },
          {
            "accuracy": 0.7505043712172159,
            "f1": 0.7457814471713863,
            "f1_weighted": 0.7462111693089635
          },
          {
            "accuracy": 0.7636180228648285,
            "f1": 0.754175232640998,
            "f1_weighted": 0.7607174268135466
          },
          {
            "accuracy": 0.7575655682582381,
            "f1": 0.7491067946517629,
            "f1_weighted": 0.7551134220589979
          },
          {
            "accuracy": 0.7363819771351715,
            "f1": 0.7311910589063159,
            "f1_weighted": 0.7306166372787689
          },
          {
            "accuracy": 0.7279757901815737,
            "f1": 0.7275025983852841,
            "f1_weighted": 0.7248716100340834
          },
          {
            "accuracy": 0.7552118359112306,
            "f1": 0.7432017673921737,
            "f1_weighted": 0.7523270597889727
          },
          {
            "accuracy": 0.7457969065232011,
            "f1": 0.7452713354966792,
            "f1_weighted": 0.7453588019062662
          },
          {
            "accuracy": 0.7824478816408877,
            "f1": 0.7765375469569651,
            "f1_weighted": 0.7774774088361666
          },
          {
            "accuracy": 0.7750504371217216,
            "f1": 0.772520078546299,
            "f1_weighted": 0.7746894365766843
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.7585833743236596,
        "f1": 0.750976458122237,
        "f1_weighted": 0.7552329606312337,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.7585833743236596,
        "scores_per_experiment": [
          {
            "accuracy": 0.7589768814559764,
            "f1": 0.752446272325823,
            "f1_weighted": 0.7567932005533703
          },
          {
            "accuracy": 0.7461878996556812,
            "f1": 0.7413218044474701,
            "f1_weighted": 0.7434603528500205
          },
          {
            "accuracy": 0.7722577471716675,
            "f1": 0.761345597506989,
            "f1_weighted": 0.7688634050629207
          },
          {
            "accuracy": 0.7530742744712248,
            "f1": 0.7441008750798291,
            "f1_weighted": 0.7482848214919452
          },
          {
            "accuracy": 0.7456960157402853,
            "f1": 0.7377843033368218,
            "f1_weighted": 0.7396580823759403
          },
          {
            "accuracy": 0.7309394982784063,
            "f1": 0.7263739502710909,
            "f1_weighted": 0.7269369244142984
          },
          {
            "accuracy": 0.7624200688637481,
            "f1": 0.7487759384438273,
            "f1_weighted": 0.7604143042265185
          },
          {
            "accuracy": 0.735858337432366,
            "f1": 0.7303500540265936,
            "f1_weighted": 0.734421778539582
          },
          {
            "accuracy": 0.7968519429414658,
            "f1": 0.7875421066196677,
            "f1_weighted": 0.7908719752208149
          },
          {
            "accuracy": 0.7835710772257747,
            "f1": 0.7797236791642576,
            "f1_weighted": 0.7826247615769252
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}