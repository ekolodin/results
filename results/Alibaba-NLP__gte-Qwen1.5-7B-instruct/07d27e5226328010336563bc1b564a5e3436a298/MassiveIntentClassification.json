{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 238.89698457717896,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.7314727639542704,
        "f1": 0.7029509613044315,
        "f1_weighted": 0.7256631827900565,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.7314727639542704,
        "scores_per_experiment": [
          {
            "accuracy": 0.7498318762609281,
            "f1": 0.716140430710699,
            "f1_weighted": 0.7451098688894293
          },
          {
            "accuracy": 0.7397444519166106,
            "f1": 0.7096092013926623,
            "f1_weighted": 0.7366102563637507
          },
          {
            "accuracy": 0.7114996637525218,
            "f1": 0.6915641707074874,
            "f1_weighted": 0.7116819647995989
          },
          {
            "accuracy": 0.7528581035642232,
            "f1": 0.7102457063724665,
            "f1_weighted": 0.742370987289208
          },
          {
            "accuracy": 0.7431069266980498,
            "f1": 0.7092041013363598,
            "f1_weighted": 0.7358669124480911
          },
          {
            "accuracy": 0.7078009414929388,
            "f1": 0.6840768024865365,
            "f1_weighted": 0.7085615317675305
          },
          {
            "accuracy": 0.726630800268998,
            "f1": 0.6986013055501684,
            "f1_weighted": 0.7156717667858077
          },
          {
            "accuracy": 0.7246133154001345,
            "f1": 0.6899712907863645,
            "f1_weighted": 0.7155703887651953
          },
          {
            "accuracy": 0.7135171486213854,
            "f1": 0.7004742109283838,
            "f1_weighted": 0.7065636857122223
          },
          {
            "accuracy": 0.7451244115669132,
            "f1": 0.7196223927731854,
            "f1_weighted": 0.7386244650797311
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.7371864240039351,
        "f1": 0.6919872164606259,
        "f1_weighted": 0.728897197686264,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.7371864240039351,
        "scores_per_experiment": [
          {
            "accuracy": 0.7481554353172651,
            "f1": 0.697706708330012,
            "f1_weighted": 0.741921738803396
          },
          {
            "accuracy": 0.7402852926709297,
            "f1": 0.6929606464768286,
            "f1_weighted": 0.7358916138155063
          },
          {
            "accuracy": 0.735858337432366,
            "f1": 0.6939703259575215,
            "f1_weighted": 0.7307368868035592
          },
          {
            "accuracy": 0.7515986227250369,
            "f1": 0.6975390996102395,
            "f1_weighted": 0.7401302137124002
          },
          {
            "accuracy": 0.7476635514018691,
            "f1": 0.6975070690946991,
            "f1_weighted": 0.736950671981587
          },
          {
            "accuracy": 0.7274963108706345,
            "f1": 0.6854731694412295,
            "f1_weighted": 0.7247630530178875
          },
          {
            "accuracy": 0.7324151500245942,
            "f1": 0.6832147716839522,
            "f1_weighted": 0.7196689872914709
          },
          {
            "accuracy": 0.7284800787014265,
            "f1": 0.6837681985370787,
            "f1_weighted": 0.7181839293197748
          },
          {
            "accuracy": 0.7147073290703394,
            "f1": 0.6814897084521937,
            "f1_weighted": 0.7028530347559647
          },
          {
            "accuracy": 0.7452041318248893,
            "f1": 0.7062424670225043,
            "f1_weighted": 0.7378718473610946
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}