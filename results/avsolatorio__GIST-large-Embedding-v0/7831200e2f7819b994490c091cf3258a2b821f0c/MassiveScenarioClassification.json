{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 55.61965775489807,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.39038332212508414,
        "f1": 0.3703014420070158,
        "f1_weighted": 0.3783183530317334,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.39038332212508414,
        "scores_per_experiment": [
          {
            "accuracy": 0.39004707464694016,
            "f1": 0.379606479019285,
            "f1_weighted": 0.37548147443710184
          },
          {
            "accuracy": 0.3765971755211836,
            "f1": 0.35876153301428837,
            "f1_weighted": 0.36086600233697647
          },
          {
            "accuracy": 0.4098856758574311,
            "f1": 0.3843082091919536,
            "f1_weighted": 0.4000994374389898
          },
          {
            "accuracy": 0.40282447881640887,
            "f1": 0.3827737864828574,
            "f1_weighted": 0.3989130503544746
          },
          {
            "accuracy": 0.39509078681909887,
            "f1": 0.3668522493066833,
            "f1_weighted": 0.37501108897551866
          },
          {
            "accuracy": 0.3776059179556153,
            "f1": 0.352398836620617,
            "f1_weighted": 0.3648601955988587
          },
          {
            "accuracy": 0.4092131809011432,
            "f1": 0.38520284513671654,
            "f1_weighted": 0.40008912654655554
          },
          {
            "accuracy": 0.3964357767316745,
            "f1": 0.3789334514347173,
            "f1_weighted": 0.38467250025139865
          },
          {
            "accuracy": 0.34162743779421656,
            "f1": 0.3281271119383458,
            "f1_weighted": 0.3273770360488625
          },
          {
            "accuracy": 0.40450571620712844,
            "f1": 0.38604991792469384,
            "f1_weighted": 0.3958136183285971
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.38607968519429414,
        "f1": 0.37169781298763793,
        "f1_weighted": 0.37306210698187925,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.38607968519429414,
        "scores_per_experiment": [
          {
            "accuracy": 0.39350713231677326,
            "f1": 0.3874975212697251,
            "f1_weighted": 0.3857069230537102
          },
          {
            "accuracy": 0.3743236596163306,
            "f1": 0.36217626897423044,
            "f1_weighted": 0.3617426074906449
          },
          {
            "accuracy": 0.4171175602557796,
            "f1": 0.3988719202101778,
            "f1_weighted": 0.40603406148637894
          },
          {
            "accuracy": 0.38317757009345793,
            "f1": 0.3700452902080645,
            "f1_weighted": 0.373892431479232
          },
          {
            "accuracy": 0.3989178553861289,
            "f1": 0.3824191495985928,
            "f1_weighted": 0.37206341511830976
          },
          {
            "accuracy": 0.36645351696999506,
            "f1": 0.3470027032770395,
            "f1_weighted": 0.35522727814418364
          },
          {
            "accuracy": 0.3836694540088539,
            "f1": 0.36567221351416934,
            "f1_weighted": 0.3748336535557361
          },
          {
            "accuracy": 0.39203148057058534,
            "f1": 0.38264509947724423,
            "f1_weighted": 0.3775186968671488
          },
          {
            "accuracy": 0.3305459911460895,
            "f1": 0.32494606817637167,
            "f1_weighted": 0.31442075266779346
          },
          {
            "accuracy": 0.42105263157894735,
            "f1": 0.3957018951707639,
            "f1_weighted": 0.40918124995565486
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}