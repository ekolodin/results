{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 94.60019040107727,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.3421318090114324,
        "f1": 0.31603976518794374,
        "f1_weighted": 0.32170964871737595,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.3421318090114324,
        "scores_per_experiment": [
          {
            "accuracy": 0.34229993275050435,
            "f1": 0.3232323049560755,
            "f1_weighted": 0.3158059737935334
          },
          {
            "accuracy": 0.33927370544720914,
            "f1": 0.311309466939703,
            "f1_weighted": 0.3235044694389404
          },
          {
            "accuracy": 0.33053127101546736,
            "f1": 0.31731342607263174,
            "f1_weighted": 0.3179582246761472
          },
          {
            "accuracy": 0.3439811701412239,
            "f1": 0.30539400822163304,
            "f1_weighted": 0.3283257579882539
          },
          {
            "accuracy": 0.35406859448554134,
            "f1": 0.3237682005772445,
            "f1_weighted": 0.3319410711079611
          },
          {
            "accuracy": 0.33187626092804307,
            "f1": 0.30085113804177466,
            "f1_weighted": 0.31973732704597224
          },
          {
            "accuracy": 0.3523873570948218,
            "f1": 0.3340003317152589,
            "f1_weighted": 0.32994936176085676
          },
          {
            "accuracy": 0.34734364492266306,
            "f1": 0.31742733272026574,
            "f1_weighted": 0.3139588620797258
          },
          {
            "accuracy": 0.32447881640887694,
            "f1": 0.30415549449099144,
            "f1_weighted": 0.30156513168903876
          },
          {
            "accuracy": 0.3550773369199731,
            "f1": 0.3229459481438593,
            "f1_weighted": 0.3343503075933291
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.3317757009345794,
        "f1": 0.30235085489219743,
        "f1_weighted": 0.3142707796407248,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.3317757009345794,
        "scores_per_experiment": [
          {
            "accuracy": 0.3305459911460895,
            "f1": 0.30437794230076715,
            "f1_weighted": 0.3117445709013516
          },
          {
            "accuracy": 0.3246433841613379,
            "f1": 0.3002974096886172,
            "f1_weighted": 0.30812844009901214
          },
          {
            "accuracy": 0.3413674372848008,
            "f1": 0.3115019204693708,
            "f1_weighted": 0.3297562868132212
          },
          {
            "accuracy": 0.33448106246925724,
            "f1": 0.2995329373559814,
            "f1_weighted": 0.32075360840894096
          },
          {
            "accuracy": 0.3605509099852435,
            "f1": 0.31890337884882075,
            "f1_weighted": 0.34352027687513015
          },
          {
            "accuracy": 0.3192326610919823,
            "f1": 0.2910237841067648,
            "f1_weighted": 0.3081223318733984
          },
          {
            "accuracy": 0.3192326610919823,
            "f1": 0.30457385838441037,
            "f1_weighted": 0.29849871962371294
          },
          {
            "accuracy": 0.33448106246925724,
            "f1": 0.2934797200024305,
            "f1_weighted": 0.3093487764595314
          },
          {
            "accuracy": 0.3044761436301033,
            "f1": 0.28085735152155133,
            "f1_weighted": 0.2814839821799514
          },
          {
            "accuracy": 0.3487456960157403,
            "f1": 0.31896024624326014,
            "f1_weighted": 0.33135080317299737
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}