{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 348.6101727485657,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.8742770679219906,
        "f1": 0.8657929115220291,
        "f1_weighted": 0.8721634824090809,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.8742770679219906,
        "scores_per_experiment": [
          {
            "accuracy": 0.8826496301277741,
            "f1": 0.8745190941798237,
            "f1_weighted": 0.8814348562423552
          },
          {
            "accuracy": 0.8735709482178884,
            "f1": 0.8643568325090288,
            "f1_weighted": 0.8709792678351427
          },
          {
            "accuracy": 0.8638197713517148,
            "f1": 0.8545573549713423,
            "f1_weighted": 0.8609202602222957
          },
          {
            "accuracy": 0.8759246805648958,
            "f1": 0.8617349512761722,
            "f1_weighted": 0.8736578511755067
          },
          {
            "accuracy": 0.8823133826496301,
            "f1": 0.8722093558537751,
            "f1_weighted": 0.879099135525846
          },
          {
            "accuracy": 0.8668459986550101,
            "f1": 0.8591478255584136,
            "f1_weighted": 0.8652445404051274
          },
          {
            "accuracy": 0.8732347007397444,
            "f1": 0.8624559964958555,
            "f1_weighted": 0.8691054052105809
          },
          {
            "accuracy": 0.8702084734364492,
            "f1": 0.866537842799181,
            "f1_weighted": 0.8697639542498959
          },
          {
            "accuracy": 0.8581035642232683,
            "f1": 0.8531640003126318,
            "f1_weighted": 0.8562544874516554
          },
          {
            "accuracy": 0.8960995292535306,
            "f1": 0.889245861264066,
            "f1_weighted": 0.8951750657724027
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.8675848499754059,
        "f1": 0.8554629390972843,
        "f1_weighted": 0.8643538702684053,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.8675848499754059,
        "scores_per_experiment": [
          {
            "accuracy": 0.8839153959665519,
            "f1": 0.8749100267713424,
            "f1_weighted": 0.8824078315059192
          },
          {
            "accuracy": 0.8652238071815052,
            "f1": 0.8534500370315077,
            "f1_weighted": 0.8624793049176476
          },
          {
            "accuracy": 0.8612887358583374,
            "f1": 0.8491228381079821,
            "f1_weighted": 0.8567239333883814
          },
          {
            "accuracy": 0.8726020659124447,
            "f1": 0.8562395512162817,
            "f1_weighted": 0.8687632432751772
          },
          {
            "accuracy": 0.8809640924741761,
            "f1": 0.8655336244135785,
            "f1_weighted": 0.877230127380317
          },
          {
            "accuracy": 0.8519429414658141,
            "f1": 0.8394940276571649,
            "f1_weighted": 0.8491416750488167
          },
          {
            "accuracy": 0.8652238071815052,
            "f1": 0.8479347109956085,
            "f1_weighted": 0.8605089335955822
          },
          {
            "accuracy": 0.8593212001967536,
            "f1": 0.8520226965958042,
            "f1_weighted": 0.8570036727091412
          },
          {
            "accuracy": 0.8558780127889818,
            "f1": 0.8470518212159359,
            "f1_weighted": 0.8516202920318172
          },
          {
            "accuracy": 0.8794884407279882,
            "f1": 0.8688700569676385,
            "f1_weighted": 0.8776596888312521
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}