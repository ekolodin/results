{"dataset_revision": "673a610d6d3dd91a547a0d57ae1b56f37ebbf6a1", "evaluation_time": 188.23047757148743, "kg_co2_emissions": null, "mteb_version": "1.15.3", "scores": {"test": [{"accuracy": 0.74520703125, "f1": 0.7307891907358736, "f1_weighted": 0.7309099552030747, "hf_subset": "default", "languages": ["rus-Cyrl"], "main_score": 0.74520703125, "scores_per_experiment": [{"accuracy": 0.74628125, "f1": 0.7318016173228531, "f1_weighted": 0.7319055576392282}, {"accuracy": 0.74969921875, "f1": 0.7351409618361473, "f1_weighted": 0.7352686554276316}, {"accuracy": 0.73114453125, "f1": 0.7150292142483131, "f1_weighted": 0.7151869375282435}, {"accuracy": 0.7492109375, "f1": 0.7349738301952348, "f1_weighted": 0.735087481916472}, {"accuracy": 0.75067578125, "f1": 0.7367660440367951, "f1_weighted": 0.7368826649904742}, {"accuracy": 0.74383984375, "f1": 0.7302542631283219, "f1_weighted": 0.7303628364685226}, {"accuracy": 0.74774609375, "f1": 0.7306573041651887, "f1_weighted": 0.7307978893704438}, {"accuracy": 0.74383984375, "f1": 0.7273070551857849, "f1_weighted": 0.7274518873927309}, {"accuracy": 0.752140625, "f1": 0.7413547916751491, "f1_weighted": 0.7414490489933776}, {"accuracy": 0.7374921875, "f1": 0.7246068255649476, "f1_weighted": 0.724706592303623}]}]}, "task_name": "RuSciBenchGRNTIClassification"}