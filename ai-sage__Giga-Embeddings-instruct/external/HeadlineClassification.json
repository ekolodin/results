{"dataset_revision": "2fe05ee6b5832cda29f2ef7aaad7b7fe6a3609eb", "evaluation_time": 119.27487397193909, "kg_co2_emissions": null, "mteb_version": "1.15.3", "scores": {"test": [{"accuracy": 0.8601484375, "f1": 0.8517323995821744, "f1_weighted": 0.8517273609511149, "hf_subset": "default", "languages": ["rus-Cyrl"], "main_score": 0.8601484375, "scores_per_experiment": [{"accuracy": 0.8390546875, "f1": 0.8310858988529732, "f1_weighted": 0.831093128291411}, {"accuracy": 0.8663984375, "f1": 0.8575532533424209, "f1_weighted": 0.8575438533546816}, {"accuracy": 0.85614453125, "f1": 0.8484491700268699, "f1_weighted": 0.8484410226975732}, {"accuracy": 0.86200390625, "f1": 0.8533053929635098, "f1_weighted": 0.8532982792561661}, {"accuracy": 0.87274609375, "f1": 0.8649271365066235, "f1_weighted": 0.8649164965450048}, {"accuracy": 0.857609375, "f1": 0.8496922977084708, "f1_weighted": 0.8496674849408536}, {"accuracy": 0.86395703125, "f1": 0.8556173814096782, "f1_weighted": 0.8556107924249053}, {"accuracy": 0.86395703125, "f1": 0.8551599807673854, "f1_weighted": 0.8551692881368087}, {"accuracy": 0.8488203125, "f1": 0.8393928418867795, "f1_weighted": 0.839392535986486}, {"accuracy": 0.87079296875, "f1": 0.8621406423570331, "f1_weighted": 0.862140727877259}]}]}, "task_name": "HeadlineClassification"}