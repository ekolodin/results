{"dataset_revision": "f6d2c31f4dc6b88f468552750bfec05b4b41b05a", "evaluation_time": 99.49517941474915, "kg_co2_emissions": null, "mteb_version": "1.15.3", "scores": {"test": [{"accuracy": 0.737150390625, "f1": 0.722246927657427, "f1_weighted": 0.7222538385584938, "hf_subset": "default", "languages": ["rus-Cyrl"], "main_score": 0.737150390625, "scores_per_experiment": [{"accuracy": 0.76727734375, "f1": 0.7556766444627057, "f1_weighted": 0.7556801472217148}, {"accuracy": 0.7335859375, "f1": 0.7208908137982117, "f1_weighted": 0.7208965683166522}, {"accuracy": 0.70770703125, "f1": 0.6920992346572302, "f1_weighted": 0.6921133052933363}, {"accuracy": 0.75751171875, "f1": 0.7502995745063169, "f1_weighted": 0.7503294558590532}, {"accuracy": 0.76337109375, "f1": 0.7562326267137918, "f1_weighted": 0.7562530032353445}, {"accuracy": 0.73114453125, "f1": 0.7076274920705875, "f1_weighted": 0.7076177281733378}, {"accuracy": 0.72528515625, "f1": 0.7018444881807873, "f1_weighted": 0.7018417559480226}, {"accuracy": 0.72626171875, "f1": 0.7139465882627896, "f1_weighted": 0.7139673545361157}, {"accuracy": 0.732609375, "f1": 0.7106761864811024, "f1_weighted": 0.7106545647425278}, {"accuracy": 0.72675, "f1": 0.7131756274407471, "f1_weighted": 0.7131845022588335}]}]}, "task_name": "RuReviewsClassification"}