{"dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285", "evaluation_time": 109.23797869682312, "kg_co2_emissions": null, "mteb_version": "1.15.3", "scores": {"test": [{"accuracy": 0.83290234375, "ap": 0.7750711273019213, "ap_weighted": 0.7750711273019213, "f1": 0.8242886423677278, "f1_weighted": 0.8242886423677278, "hf_subset": "default", "languages": ["rus-Cyrl"], "main_score": 0.83290234375, "scores_per_experiment": [{"accuracy": 0.84149609375, "ap": 0.7736859961854461, "ap_weighted": 0.7736859961854461, "f1": 0.8334293352872415, "f1_weighted": 0.8334293352872415}, {"accuracy": 0.83368359375, "ap": 0.7638794513081395, "ap_weighted": 0.7638794513081395, "f1": 0.8255754284117934, "f1_weighted": 0.8255754284117934}, {"accuracy": 0.84833203125, "ap": 0.7986662462757312, "ap_weighted": 0.7986662462757312, "f1": 0.8399427547553203, "f1_weighted": 0.8399427547553203}, {"accuracy": 0.81317578125, "ap": 0.7630947639049827, "ap_weighted": 0.7630947639049827, "f1": 0.8039642710374743, "f1_weighted": 0.8039642710374743}, {"accuracy": 0.81610546875, "ap": 0.7798060013745148, "ap_weighted": 0.7798060013745148, "f1": 0.805179140818284, "f1_weighted": 0.805179140818284}, {"accuracy": 0.83368359375, "ap": 0.7726653343023256, "ap_weighted": 0.7726653343023256, "f1": 0.8256326675457342, "f1_weighted": 0.8256326675457342}, {"accuracy": 0.8624921875, "ap": 0.8061846348947895, "ap_weighted": 0.8061846348947895, "f1": 0.854468732085917, "f1_weighted": 0.854468732085917}, {"accuracy": 0.83221875, "ap": 0.7582170758928571, "ap_weighted": 0.7582170758928571, "f1": 0.8238316613653046, "f1_weighted": 0.8238316613653046}, {"accuracy": 0.8371015625, "ap": 0.7920303520114942, "ap_weighted": 0.7920303520114942, "f1": 0.828129750529182, "f1_weighted": 0.828129750529182}, {"accuracy": 0.810734375, "ap": 0.742481416868932, "ap_weighted": 0.742481416868932, "f1": 0.8027326818410268, "f1_weighted": 0.8027326818410268}]}]}, "task_name": "InappropriatenessClassification"}